{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining Provincial Weather Data and Engineering Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weather data from https://agriculture.alberta.ca/acis/weather-data-viewer.jsp\n",
    "# different weather stations were used for different field ID's, based on latitude and longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spore data, weather from field weather stations, field ID information\n",
    "df = pd.read_csv('data_and_location2.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FieldID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Year_x</th>\n",
       "      <th>JDay</th>\n",
       "      <th>SsCtMean</th>\n",
       "      <th>SsCtSD</th>\n",
       "      <th>SsMean</th>\n",
       "      <th>logSsMean</th>\n",
       "      <th>TtCt</th>\n",
       "      <th>TtSD</th>\n",
       "      <th>...</th>\n",
       "      <th>Year_y</th>\n",
       "      <th>SamplerNo</th>\n",
       "      <th>CropType</th>\n",
       "      <th>Crop</th>\n",
       "      <th>Region</th>\n",
       "      <th>Province</th>\n",
       "      <th>SamplerPresent</th>\n",
       "      <th>SamplerType</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1904</td>\n",
       "      <td>2019-07-05</td>\n",
       "      <td>2019</td>\n",
       "      <td>186</td>\n",
       "      <td>37.15</td>\n",
       "      <td>0.63</td>\n",
       "      <td>6.51</td>\n",
       "      <td>0.875640</td>\n",
       "      <td>23.60</td>\n",
       "      <td>0.10</td>\n",
       "      <td>...</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bean</td>\n",
       "      <td>Bean</td>\n",
       "      <td>Vauxhall</td>\n",
       "      <td>Alberta</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Burkard</td>\n",
       "      <td>49.869673</td>\n",
       "      <td>-112.076528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1904</td>\n",
       "      <td>2019-07-06</td>\n",
       "      <td>2019</td>\n",
       "      <td>187</td>\n",
       "      <td>40.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.29</td>\n",
       "      <td>0.07</td>\n",
       "      <td>...</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bean</td>\n",
       "      <td>Bean</td>\n",
       "      <td>Vauxhall</td>\n",
       "      <td>Alberta</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Burkard</td>\n",
       "      <td>49.869673</td>\n",
       "      <td>-112.076528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1904</td>\n",
       "      <td>2019-07-07</td>\n",
       "      <td>2019</td>\n",
       "      <td>188</td>\n",
       "      <td>36.85</td>\n",
       "      <td>0.25</td>\n",
       "      <td>7.57</td>\n",
       "      <td>0.932981</td>\n",
       "      <td>23.40</td>\n",
       "      <td>0.07</td>\n",
       "      <td>...</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bean</td>\n",
       "      <td>Bean</td>\n",
       "      <td>Vauxhall</td>\n",
       "      <td>Alberta</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Burkard</td>\n",
       "      <td>49.869673</td>\n",
       "      <td>-112.076528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1904</td>\n",
       "      <td>2019-07-08</td>\n",
       "      <td>2019</td>\n",
       "      <td>189</td>\n",
       "      <td>38.80</td>\n",
       "      <td>0.76</td>\n",
       "      <td>2.32</td>\n",
       "      <td>0.521138</td>\n",
       "      <td>24.03</td>\n",
       "      <td>0.05</td>\n",
       "      <td>...</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bean</td>\n",
       "      <td>Bean</td>\n",
       "      <td>Vauxhall</td>\n",
       "      <td>Alberta</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Burkard</td>\n",
       "      <td>49.869673</td>\n",
       "      <td>-112.076528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1904</td>\n",
       "      <td>2019-07-09</td>\n",
       "      <td>2019</td>\n",
       "      <td>190</td>\n",
       "      <td>33.44</td>\n",
       "      <td>0.25</td>\n",
       "      <td>65.06</td>\n",
       "      <td>1.819939</td>\n",
       "      <td>24.41</td>\n",
       "      <td>0.08</td>\n",
       "      <td>...</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bean</td>\n",
       "      <td>Bean</td>\n",
       "      <td>Vauxhall</td>\n",
       "      <td>Alberta</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Burkard</td>\n",
       "      <td>49.869673</td>\n",
       "      <td>-112.076528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  FieldID        Date  Year_x  JDay SsCtMean  SsCtSD  SsMean  logSsMean  \\\n",
       "0    1904  2019-07-05    2019   186    37.15    0.63    6.51   0.875640   \n",
       "1    1904  2019-07-06    2019   187    40.00    0.00    0.00   0.000000   \n",
       "2    1904  2019-07-07    2019   188    36.85    0.25    7.57   0.932981   \n",
       "3    1904  2019-07-08    2019   189    38.80    0.76    2.32   0.521138   \n",
       "4    1904  2019-07-09    2019   190    33.44    0.25   65.06   1.819939   \n",
       "\n",
       "    TtCt  TtSD  ...  Year_y  SamplerNo  CropType  Crop    Region  Province  \\\n",
       "0  23.60  0.10  ...  2019.0        NaN      Bean  Bean  Vauxhall   Alberta   \n",
       "1  23.29  0.07  ...  2019.0        NaN      Bean  Bean  Vauxhall   Alberta   \n",
       "2  23.40  0.07  ...  2019.0        NaN      Bean  Bean  Vauxhall   Alberta   \n",
       "3  24.03  0.05  ...  2019.0        NaN      Bean  Bean  Vauxhall   Alberta   \n",
       "4  24.41  0.08  ...  2019.0        NaN      Bean  Bean  Vauxhall   Alberta   \n",
       "\n",
       "   SamplerPresent  SamplerType        Lat        Long  \n",
       "0             Yes      Burkard  49.869673 -112.076528  \n",
       "1             Yes      Burkard  49.869673 -112.076528  \n",
       "2             Yes      Burkard  49.869673 -112.076528  \n",
       "3             Yes      Burkard  49.869673 -112.076528  \n",
       "4             Yes      Burkard  49.869673 -112.076528  \n",
       "\n",
       "[5 rows x 85 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df sorted by date so we can id earliest and latest dates for weather history\n",
    "df2 = df.copy()\n",
    "df2['Date'] = pd.to_datetime(df2['Date'])\n",
    "df2 = df.sort_values(by='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "905    2018-06-13\n",
      "Name: Date, dtype: object\n",
      "750    2020-08-30\n",
      "Name: Date, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df2.Date.head(1))\n",
    "print(df2.Date.tail(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Vauxhall', 'Cranford', 'Taber', '1919', '1920', '1921', '1906',\n",
       "       '1910', '1922', '1904', '1917', '1915', '2007', '2006', '2042',\n",
       "       '2001', '2004'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2018 dates start on June 13, 2020 dates end on Aug 30\n",
    "df2['FieldID'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matching Provincial Data to Field Sites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weather stations: Field ID's** \\\n",
    "rolling hills weather: 'vauxhall' \\\n",
    "barnwell weather: 'cranford' \\\n",
    "lethbridge weather: 1919, 1920, 1921, 1922, 2042 \\\n",
    "vauxhall weather: 1906, 1910, 2007 \\\n",
    "fincastle weather: 1904 \\\n",
    "bow weather = 1917, 2006, 2004, 'taber' \\\n",
    "bow north weather = 1915 \\\n",
    "grassy lake weather: 2001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_hills1 = pd.read_csv('rolling_hills_18-19.csv')\n",
    "rolling_hills2 = pd.read_csv('rolling_hills_20.csv')\n",
    "rolling_hills = pd.concat([rolling_hills1, rolling_hills2], ignore_index=True)\n",
    "weather_dfs.append(rolling_hills)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "barnwell1 = pd.read_csv('barnwell_18-19.csv')\n",
    "barnwell2 = pd.read_csv('barnwell_20.csv')\n",
    "barnwell = pd.concat([barnwell1, barnwell2], ignore_index=True)\n",
    "weather_dfs.append(barnwell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "lethbridge1 = pd.read_csv('lethbridge_18-19.csv')\n",
    "lethbridge2 = pd.read_csv('lethbridge_20.csv')\n",
    "lethbridge = pd.concat([lethbridge1, lethbridge2], ignore_index=True)\n",
    "weather_dfs.append(lethbridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vauxhall1 = pd.read_csv('vauxhall_18-19.csv')\n",
    "vauxhall2 = pd.read_csv('vauxhall_20.csv')\n",
    "vauxhall = pd.concat([vauxhall1, vauxhall2], ignore_index=True)\n",
    "weather_dfs.append(vauxhall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "fincastle1 = pd.read_csv('fincastle_18-19.csv')\n",
    "fincastle2 = pd.read_csv('fincastle_20.csv')\n",
    "fincastle = pd.concat([fincastle1, fincastle2], ignore_index=True)\n",
    "weather_dfs.append(fincastle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow1 = pd.read_csv('bow_18-19.csv')\n",
    "bow2 = pd.read_csv('bow_20.csv')\n",
    "bow = pd.concat([bow1, bow2], ignore_index=True)\n",
    "weather_dfs.append(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_north1 = pd.read_csv('bow_north_18-19.csv')\n",
    "bow_north2 = pd.read_csv('bow_north_20.csv')\n",
    "bow_north = pd.concat([bow_north1, bow_north2], ignore_index=True)\n",
    "weather_dfs.append(bow_north)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "grassy_lake1 = pd.read_csv('grassy_lake_18-19.csv')\n",
    "grassy_lake2 = pd.read_csv('grassy_lake_20.csv')\n",
    "grassy_lake = pd.concat([grassy_lake1, grassy_lake2], ignore_index=True)\n",
    "weather_dfs.append(grassy_lake)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Engineer Features for each weather location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calucluate dew point from relative humidity and temperature\n",
    "def dew_point(RH, Temp):\n",
    "    a = Temp\n",
    "    b = ((100-RH)/5)\n",
    "    DP = a-b\n",
    "    return DP\n",
    "\n",
    "# function to calculate rolling means\n",
    "# weather data extends several weeks before first spore data point, so nan values do not need to be imputed, as they will be dropped\n",
    "def rolling_mean(data, col):\n",
    "    # -1 day data\n",
    "    name1 = col + '_1'\n",
    "    # shift by 1\n",
    "    lag1 = data[col].shift(1)\n",
    "    # assign to column  \n",
    "    data[name1] = lag1\n",
    "    \n",
    "    # -3 day mean\n",
    "    name3 = col + '_3'\n",
    "    # assign mean to column\n",
    "    window3 = lag1.rolling(window=3).mean()\n",
    "    data[name3] = window3\n",
    "    \n",
    "    # 7 day rolling mean leads to a lot of lost values\n",
    "    # -7 day mean\n",
    "    name7 = col + '_7'\n",
    "    # assign mean to column\n",
    "    window7 = lag1.rolling(window=7).mean()\n",
    "    data[name7] = window7\n",
    "    \n",
    "    # return new dataframe\n",
    "    return data\n",
    "\n",
    "# function to calculate rolling sums\n",
    "def rolling_sum(data, col):\n",
    "    # -1 day data\n",
    "    name1 = col + 'Sum_1'\n",
    "    # shift by 1\n",
    "    lag1 = data[col].shift(1)\n",
    "    # assign to column  \n",
    "    data[name1] = lag1\n",
    "    \n",
    "    # -3 day mean\n",
    "    name3 = col + 'Sum_3'\n",
    "    # assign mean to column\n",
    "    window3 = lag1.rolling(window=3).sum()\n",
    "    data[name3] = window3\n",
    "    \n",
    "    # 7 day rolling mean leads to a lot of lost values\n",
    "    # -7 day mean\n",
    "    name7 = col + 'Sum_7'\n",
    "    # assign mean to column\n",
    "    window7 = lag1.rolling(window=7).sum()\n",
    "    data[name7] = window7\n",
    "    \n",
    "    # return new dataframe\n",
    "    return data\n",
    "\n",
    "# function to label temperature features as under dew point or not\n",
    "def temp_DP(data, col):\n",
    "    name = col + '_uDP'\n",
    "    # 1 = under dew point\n",
    "    data[name] = data.apply(lambda x: 1 if x[col] <= x['MeanDP'] else 0, axis = 1)\n",
    "    return data\n",
    "\n",
    "# function to label temperature features as under 20 degrees or not\n",
    "def cool_temp(data, col):\n",
    "    name = col + '_u20'\n",
    "    # 1 = under 20\n",
    "    data[name] = data[col].apply(lambda x: 1 if x <= 20 else 0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap all feature engineering into 1 function\n",
    "def engineer(data):\n",
    "    # drop extra columns\n",
    "    to_drop = ['Air Temp. Min. Source Flag', 'Air Temp. Min. Record Completeness (%)',\n",
    "       'Air Temp. Max. Source Flag','Air Temp. Max. Record Completeness (%)', \n",
    "       'Air Temp. Avg. Source Flag', 'Air Temp. Avg. Record Completeness (%)',\n",
    "       'Humidity Avg. Source Flag','Humidity Avg. Record Completeness (%)', \n",
    "       'Precip. Accumulated Source Flag', 'Precip. Accumulated Comment','Precip. Source Flag', 'Precip. Comment',\n",
    "        'Wind Speed 2 m Avg. Source Flag','Wind Speed 2 m Avg. Record Completeness (%)',\n",
    "        'Wind Speed 10 m Avg. Source Flag','Wind Speed 10 m Avg. Record Completeness (%)',\n",
    "        'Wind Dir. 10 m Avg. Source Flag','Wind Dir. 10 m Avg. Record Completeness (%)',\n",
    "       'Frost Probability 0�C Source Flag','Frost Probability 0�C Record Completeness (%)', \n",
    "        'Precip. Accumulated (mm)']\n",
    "    data.drop(labels=to_drop, axis=1, inplace=True)\n",
    "    # rename columns\n",
    "    data.columns = ['Station','Date','MinTemp','MaxTemp','MeanTemp','MeanRH','Precip','Wind2','Wind10','Wind10Dir','Frost']\n",
    "    # turn date column into datetime\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "    # create dew point feature\n",
    "    data['MeanDP'] = dew_point(data['MeanRH'], data['MeanTemp'])\n",
    "    # create counts below dewpoint and 20 degrees\n",
    "    temp_cols = ['MinTemp', 'MaxTemp', 'MeanTemp']\n",
    "    for col in temp_cols:\n",
    "        data = temp_DP(data, col)\n",
    "        data = cool_temp(data, col)\n",
    "    # calculate rolling means and sums\n",
    "    roll_means = ['MinTemp','MaxTemp','MeanTemp','MeanRH','Precip','Wind2','Wind10','Wind10Dir','Frost','MeanDP']\n",
    "    roll_sums = ['Precip','MinTemp_uDP','MaxTemp_uDP','MeanTemp_uDP', 'MinTemp_u20', 'MaxTemp_u20', 'MeanTemp_u20']\n",
    "    for col in roll_means:\n",
    "        data = rolling_mean(data, col)\n",
    "    for col in roll_sums:\n",
    "        data = rolling_sum(data, col)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# engineer features for each weather location\n",
    "location_names = ['rolling_hills','barnwell','lethbridge','vauxhall','fincastle','bow','bow_north','grassy_lake']\n",
    "for frame in weather_dfs:\n",
    "    index = 0\n",
    "    frame = engineer(frame)\n",
    "    filename = location_names[index]\n",
    "    frame.to_csv(filename + '2_processed.csv')\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(600, 69)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rolling_hills.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Station</th>\n",
       "      <th>Date</th>\n",
       "      <th>MinTemp</th>\n",
       "      <th>MaxTemp</th>\n",
       "      <th>MeanTemp</th>\n",
       "      <th>MeanRH</th>\n",
       "      <th>Precip</th>\n",
       "      <th>Wind2</th>\n",
       "      <th>Wind10</th>\n",
       "      <th>Wind10Dir</th>\n",
       "      <th>...</th>\n",
       "      <th>MeanTemp_uDPSum_7</th>\n",
       "      <th>MinTemp_u20Sum_1</th>\n",
       "      <th>MinTemp_u20Sum_3</th>\n",
       "      <th>MinTemp_u20Sum_7</th>\n",
       "      <th>MaxTemp_u20Sum_1</th>\n",
       "      <th>MaxTemp_u20Sum_3</th>\n",
       "      <th>MaxTemp_u20Sum_7</th>\n",
       "      <th>MeanTemp_u20Sum_1</th>\n",
       "      <th>MeanTemp_u20Sum_3</th>\n",
       "      <th>MeanTemp_u20Sum_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rolling Hills AGCM</td>\n",
       "      <td>2018-05-27</td>\n",
       "      <td>5.6</td>\n",
       "      <td>27.9</td>\n",
       "      <td>18.7</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.9</td>\n",
       "      <td>11.5</td>\n",
       "      <td>68.1</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rolling Hills AGCM</td>\n",
       "      <td>2018-05-28</td>\n",
       "      <td>9.5</td>\n",
       "      <td>31.6</td>\n",
       "      <td>21.8</td>\n",
       "      <td>44.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.3</td>\n",
       "      <td>20.1</td>\n",
       "      <td>182.7</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rolling Hills AGCM</td>\n",
       "      <td>2018-05-29</td>\n",
       "      <td>11.1</td>\n",
       "      <td>22.1</td>\n",
       "      <td>17.3</td>\n",
       "      <td>42.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.9</td>\n",
       "      <td>25.3</td>\n",
       "      <td>341.5</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rolling Hills AGCM</td>\n",
       "      <td>2018-05-30</td>\n",
       "      <td>7.9</td>\n",
       "      <td>20.4</td>\n",
       "      <td>13.0</td>\n",
       "      <td>65.4</td>\n",
       "      <td>6.1</td>\n",
       "      <td>13.0</td>\n",
       "      <td>15.9</td>\n",
       "      <td>344.6</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rolling Hills AGCM</td>\n",
       "      <td>2018-05-31</td>\n",
       "      <td>4.2</td>\n",
       "      <td>10.2</td>\n",
       "      <td>8.5</td>\n",
       "      <td>92.5</td>\n",
       "      <td>5.2</td>\n",
       "      <td>14.8</td>\n",
       "      <td>18.2</td>\n",
       "      <td>40.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Station       Date  MinTemp  MaxTemp  MeanTemp  MeanRH  Precip  \\\n",
       "0  Rolling Hills AGCM 2018-05-27      5.6     27.9      18.7    57.0     0.0   \n",
       "1  Rolling Hills AGCM 2018-05-28      9.5     31.6      21.8    44.8     0.0   \n",
       "2  Rolling Hills AGCM 2018-05-29     11.1     22.1      17.3    42.3     0.0   \n",
       "3  Rolling Hills AGCM 2018-05-30      7.9     20.4      13.0    65.4     6.1   \n",
       "4  Rolling Hills AGCM 2018-05-31      4.2     10.2       8.5    92.5     5.2   \n",
       "\n",
       "   Wind2  Wind10  Wind10Dir  ...  MeanTemp_uDPSum_7  MinTemp_u20Sum_1  \\\n",
       "0    9.9    11.5       68.1  ...                NaN               NaN   \n",
       "1   16.3    20.1      182.7  ...                NaN               1.0   \n",
       "2   19.9    25.3      341.5  ...                NaN               1.0   \n",
       "3   13.0    15.9      344.6  ...                NaN               1.0   \n",
       "4   14.8    18.2       40.0  ...                NaN               1.0   \n",
       "\n",
       "   MinTemp_u20Sum_3  MinTemp_u20Sum_7  MaxTemp_u20Sum_1  MaxTemp_u20Sum_3  \\\n",
       "0               NaN               NaN               NaN               NaN   \n",
       "1               NaN               NaN               0.0               NaN   \n",
       "2               NaN               NaN               0.0               NaN   \n",
       "3               3.0               NaN               0.0               0.0   \n",
       "4               3.0               NaN               0.0               0.0   \n",
       "\n",
       "   MaxTemp_u20Sum_7  MeanTemp_u20Sum_1  MeanTemp_u20Sum_3  MeanTemp_u20Sum_7  \n",
       "0               NaN                NaN                NaN                NaN  \n",
       "1               NaN                1.0                NaN                NaN  \n",
       "2               NaN                0.0                NaN                NaN  \n",
       "3               NaN                1.0                2.0                NaN  \n",
       "4               NaN                1.0                2.0                NaN  \n",
       "\n",
       "[5 rows x 69 columns]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rolling_hills.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge provincial weather location data with spore dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse spore df into location groups to correspond to provincial data\n",
    "df_rolling_hills = df2[df2['FieldID'] == 'Vauxhall'].copy()\n",
    "df_barnwell = df2[df2['FieldID'] == 'Cranford'].copy()\n",
    "df_lethbridge = df2[(df2['FieldID'] == '1919') | (df2['FieldID'] == '1920') | (df2['FieldID'] == '1921') | (df2['FieldID'] == '1922') | (df2['FieldID'] == '2042')].copy()\n",
    "df_vauxhall = df2[(df2['FieldID'] == '1906') | (df2['FieldID'] == '1910') | (df2['FieldID'] == '2007')].copy()\n",
    "df_fincastle = df2[df2['FieldID'] == '1904'].copy()\n",
    "df_bow = df2[(df2['FieldID'] == '1917') | (df2['FieldID'] == '2006') | (df2['FieldID'] == '2004') | (df2['FieldID'] == 'Taber')].copy()\n",
    "df_bow_north = df2[df2['FieldID'] == '1915'].copy()\n",
    "df_grassy_lake = df2[df2['FieldID'] == '2001'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77, 85)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rolling_hills.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FieldID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Year_x</th>\n",
       "      <th>JDay</th>\n",
       "      <th>SsCtMean</th>\n",
       "      <th>SsCtSD</th>\n",
       "      <th>SsMean</th>\n",
       "      <th>logSsMean</th>\n",
       "      <th>TtCt</th>\n",
       "      <th>TtSD</th>\n",
       "      <th>...</th>\n",
       "      <th>Year_y</th>\n",
       "      <th>SamplerNo</th>\n",
       "      <th>CropType</th>\n",
       "      <th>Crop</th>\n",
       "      <th>Region</th>\n",
       "      <th>Province</th>\n",
       "      <th>SamplerPresent</th>\n",
       "      <th>SamplerType</th>\n",
       "      <th>Lat</th>\n",
       "      <th>Long</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>905</th>\n",
       "      <td>Vauxhall</td>\n",
       "      <td>2018-06-13</td>\n",
       "      <td>2018</td>\n",
       "      <td>164</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>City</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vauxhall</td>\n",
       "      <td>Alberta</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.070242</td>\n",
       "      <td>-112.106166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>Vauxhall</td>\n",
       "      <td>2018-06-14</td>\n",
       "      <td>2018</td>\n",
       "      <td>165</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>City</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vauxhall</td>\n",
       "      <td>Alberta</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.070242</td>\n",
       "      <td>-112.106166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>907</th>\n",
       "      <td>Vauxhall</td>\n",
       "      <td>2018-06-15</td>\n",
       "      <td>2018</td>\n",
       "      <td>166</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>City</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vauxhall</td>\n",
       "      <td>Alberta</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.070242</td>\n",
       "      <td>-112.106166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>908</th>\n",
       "      <td>Vauxhall</td>\n",
       "      <td>2018-06-16</td>\n",
       "      <td>2018</td>\n",
       "      <td>167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>City</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vauxhall</td>\n",
       "      <td>Alberta</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.070242</td>\n",
       "      <td>-112.106166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>909</th>\n",
       "      <td>Vauxhall</td>\n",
       "      <td>2018-06-17</td>\n",
       "      <td>2018</td>\n",
       "      <td>168</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.95</td>\n",
       "      <td>1.202761</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>City</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Vauxhall</td>\n",
       "      <td>Alberta</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.070242</td>\n",
       "      <td>-112.106166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      FieldID       Date  Year_x  JDay SsCtMean  SsCtSD  SsMean  logSsMean  \\\n",
       "905  Vauxhall 2018-06-13    2018   164      NaN     NaN     NaN        NaN   \n",
       "906  Vauxhall 2018-06-14    2018   165      NaN     NaN     NaN        NaN   \n",
       "907  Vauxhall 2018-06-15    2018   166      NaN     NaN     NaN        NaN   \n",
       "908  Vauxhall 2018-06-16    2018   167      NaN     NaN     NaN        NaN   \n",
       "909  Vauxhall 2018-06-17    2018   168      NaN     NaN   14.95   1.202761   \n",
       "\n",
       "     TtCt  TtSD  ...  Year_y  SamplerNo  CropType  Crop    Region  Province  \\\n",
       "905   NaN   NaN  ...  2019.0        NaN      City   NaN  Vauxhall   Alberta   \n",
       "906   NaN   NaN  ...  2019.0        NaN      City   NaN  Vauxhall   Alberta   \n",
       "907   NaN   NaN  ...  2019.0        NaN      City   NaN  Vauxhall   Alberta   \n",
       "908   NaN   NaN  ...  2019.0        NaN      City   NaN  Vauxhall   Alberta   \n",
       "909   NaN   NaN  ...  2019.0        NaN      City   NaN  Vauxhall   Alberta   \n",
       "\n",
       "     SamplerPresent  SamplerType        Lat        Long  \n",
       "905              No          NaN  50.070242 -112.106166  \n",
       "906              No          NaN  50.070242 -112.106166  \n",
       "907              No          NaN  50.070242 -112.106166  \n",
       "908              No          NaN  50.070242 -112.106166  \n",
       "909              No          NaN  50.070242 -112.106166  \n",
       "\n",
       "[5 rows x 85 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rolling_hills.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge provincial weather data to appropriate spore df based on date\n",
    "data_dfs = [df_rolling_hills, df_barnwell, df_lethbridge,df_vauxhall, df_fincastle, df_bow, df_bow_north, df_grassy_lake]\n",
    "\n",
    "for i in range(len(data_dfs)):\n",
    "    frame = data_dfs[i]\n",
    "    weather = weather_dfs[i]\n",
    "    frame = frame.merge(weather, how='left', on='Date', copy=False)\n",
    "    data_dfs[i] = frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77, 153)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dfs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge location dataframes back into one dataframe\n",
    "main = pd.concat(data_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(982, 153)"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FieldID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Year_x</th>\n",
       "      <th>JDay</th>\n",
       "      <th>SsCtMean</th>\n",
       "      <th>SsCtSD</th>\n",
       "      <th>SsMean</th>\n",
       "      <th>logSsMean</th>\n",
       "      <th>TtCt</th>\n",
       "      <th>TtSD</th>\n",
       "      <th>...</th>\n",
       "      <th>MeanTemp_uDPSum_7</th>\n",
       "      <th>MinTemp_u20Sum_1</th>\n",
       "      <th>MinTemp_u20Sum_3</th>\n",
       "      <th>MinTemp_u20Sum_7</th>\n",
       "      <th>MaxTemp_u20Sum_1</th>\n",
       "      <th>MaxTemp_u20Sum_3</th>\n",
       "      <th>MaxTemp_u20Sum_7</th>\n",
       "      <th>MeanTemp_u20Sum_1</th>\n",
       "      <th>MeanTemp_u20Sum_3</th>\n",
       "      <th>MeanTemp_u20Sum_7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>2001</td>\n",
       "      <td>2020-08-16</td>\n",
       "      <td>2020</td>\n",
       "      <td>229</td>\n",
       "      <td>25.19</td>\n",
       "      <td>0.028</td>\n",
       "      <td>8432.0</td>\n",
       "      <td>3.926</td>\n",
       "      <td>26.80</td>\n",
       "      <td>0.023</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>2001</td>\n",
       "      <td>2020-08-17</td>\n",
       "      <td>2020</td>\n",
       "      <td>230</td>\n",
       "      <td>25.95</td>\n",
       "      <td>0.057</td>\n",
       "      <td>4851.0</td>\n",
       "      <td>3.686</td>\n",
       "      <td>26.79</td>\n",
       "      <td>0.013</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>2001</td>\n",
       "      <td>2020-08-18</td>\n",
       "      <td>2020</td>\n",
       "      <td>231</td>\n",
       "      <td>23.10</td>\n",
       "      <td>0.038</td>\n",
       "      <td>38199.0</td>\n",
       "      <td>4.582</td>\n",
       "      <td>26.78</td>\n",
       "      <td>0.018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>2001</td>\n",
       "      <td>2020-08-19</td>\n",
       "      <td>2020</td>\n",
       "      <td>232</td>\n",
       "      <td>23.38</td>\n",
       "      <td>0.265</td>\n",
       "      <td>31535.0</td>\n",
       "      <td>4.499</td>\n",
       "      <td>26.90</td>\n",
       "      <td>0.041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2001</td>\n",
       "      <td>2020-08-20</td>\n",
       "      <td>2020</td>\n",
       "      <td>233</td>\n",
       "      <td>26.62</td>\n",
       "      <td>0.084</td>\n",
       "      <td>2999.0</td>\n",
       "      <td>3.477</td>\n",
       "      <td>26.83</td>\n",
       "      <td>0.044</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 153 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   FieldID       Date  Year_x  JDay SsCtMean  SsCtSD   SsMean  logSsMean  \\\n",
       "32    2001 2020-08-16    2020   229    25.19   0.028   8432.0      3.926   \n",
       "33    2001 2020-08-17    2020   230    25.95   0.057   4851.0      3.686   \n",
       "34    2001 2020-08-18    2020   231    23.10   0.038  38199.0      4.582   \n",
       "35    2001 2020-08-19    2020   232    23.38   0.265  31535.0      4.499   \n",
       "36    2001 2020-08-20    2020   233    26.62   0.084   2999.0      3.477   \n",
       "\n",
       "     TtCt   TtSD  ...  MeanTemp_uDPSum_7  MinTemp_u20Sum_1  MinTemp_u20Sum_3  \\\n",
       "32  26.80  0.023  ...                0.0               1.0               3.0   \n",
       "33  26.79  0.013  ...                0.0               1.0               3.0   \n",
       "34  26.78  0.018  ...                0.0               1.0               3.0   \n",
       "35  26.90  0.041  ...                0.0               1.0               3.0   \n",
       "36  26.83  0.044  ...                0.0               1.0               3.0   \n",
       "\n",
       "    MinTemp_u20Sum_7  MaxTemp_u20Sum_1  MaxTemp_u20Sum_3  MaxTemp_u20Sum_7  \\\n",
       "32               7.0               0.0               0.0               0.0   \n",
       "33               7.0               0.0               0.0               0.0   \n",
       "34               7.0               0.0               0.0               0.0   \n",
       "35               7.0               0.0               0.0               0.0   \n",
       "36               7.0               0.0               0.0               0.0   \n",
       "\n",
       "    MeanTemp_u20Sum_1  MeanTemp_u20Sum_3  MeanTemp_u20Sum_7  \n",
       "32                1.0                3.0                7.0  \n",
       "33                0.0                2.0                6.0  \n",
       "34                0.0                1.0                5.0  \n",
       "35                0.0                0.0                4.0  \n",
       "36                0.0                0.0                3.0  \n",
       "\n",
       "[5 rows x 153 columns]"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows where target is missing values\n",
    "main.dropna(subset=['SsMean'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main.SsMean.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(933, 153)"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "main.to_csv('final_df2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Several Datasets to try during modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Provincial weather data, with rolling means of spore counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Despite high number of missing values, I also tried engineering features using the field weather station data\n",
    "# model performace was not improved by using this data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# customize rolling_mean function for spore counts\n",
    "# Dropping rows with nan values led to lower model performance, so rolling mean nan values were imputed instead\n",
    "def spore_rolling_mean(data, col):\n",
    "    # drop nan SsMean values\n",
    "    data.dropna(subset=[col], inplace=True)\n",
    "    \n",
    "    # -1 day data\n",
    "    name1 = col + '_1'\n",
    "    # shift by 1\n",
    "    lag1 = data[col].shift(1)\n",
    "    # assign to column  \n",
    "    data[name1] = lag1\n",
    "    \n",
    "    # -3 day mean\n",
    "    name3 = col + '_3'\n",
    "    # assign mean to column\n",
    "    window3 = lag1.rolling(window=3).mean()\n",
    "    data[name3] = window3\n",
    "    \n",
    "    # 7 day rolling mean leads to a lot of lost values\n",
    "    # -7 day mean\n",
    "    name7 = col + '_7'\n",
    "    # assign mean to column\n",
    "    window7 = lag1.rolling(window=7).mean()\n",
    "    data[name7] = window7\n",
    "    \n",
    "    # fill NaN's with mean info so that we can keep as much data as possible\n",
    "    # fill 1 day lag Nan with SsMean of first day\n",
    "    data[name1] = data[name1].fillna(value=data.iloc[0][col])\n",
    "    # fill 3 day lag Nans with value of 1 lag\n",
    "    data[name3] = data[name3].fillna(value=data[name1])\n",
    "    # fill 3 day lag Nans with value of rolling 3 day\n",
    "    data[name7] = data[name7].fillna(value=data[name3])\n",
    "    \n",
    "    # return new dataframe\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate spore rolling mean features\n",
    "# Note: spore rolling means were also calculated based on Field ID (instead of prov. weather station id), but lower model performace was observed\n",
    "for i in range(len(data_dfs)):\n",
    "    frame = data_dfs[i]\n",
    "    frame = spore_rolling_mean(frame, 'SsMean')\n",
    "    data_dfs[i] = frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(166, 156)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dfs[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge dataframes back into into one df\n",
    "main_with_spores = pd.concat(data_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(933, 156)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_with_spores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep provincial weather features\n",
    "to_keep = ['FieldID','Date','JDay','SsMean','Crop',\n",
    " 'MinTemp_y','MaxTemp_y','MeanTemp_y','MeanRH_y','Precip','Wind2','Wind10','Wind10Dir','Frost','MeanDP_y',\n",
    " 'MinTemp_uDP','MinTemp_u20','MaxTemp_uDP','MaxTemp_u20','MeanTemp_uDP','MeanTemp_u20','MinTemp_1','MinTemp_3',\n",
    " 'MinTemp_7','MaxTemp_1','MaxTemp_3','MaxTemp_7','MeanTemp_1','MeanTemp_3','MeanTemp_7','MeanRH_1',\n",
    " 'MeanRH_3','MeanRH_7','Precip_1','Precip_3','Precip_7','Wind2_1','Wind2_3','Wind2_7','Wind10_1',\n",
    " 'Wind10_3','Wind10_7','Wind10Dir_1','Wind10Dir_3','Wind10Dir_7','Frost_1','Frost_3','Frost_7',\n",
    " 'MeanDP_1','MeanDP_3','MeanDP_7','PrecipSum_1','PrecipSum_3','PrecipSum_7','MinTemp_uDPSum_1','MinTemp_uDPSum_3',\n",
    " 'MinTemp_uDPSum_7','MaxTemp_uDPSum_1','MaxTemp_uDPSum_3','MaxTemp_uDPSum_7','MeanTemp_uDPSum_1','MeanTemp_uDPSum_3',\n",
    " 'MeanTemp_uDPSum_7','MinTemp_u20Sum_1','MinTemp_u20Sum_3','MinTemp_u20Sum_7','MaxTemp_u20Sum_1','MaxTemp_u20Sum_3','MaxTemp_u20Sum_7',\n",
    " 'MeanTemp_u20Sum_1','MeanTemp_u20Sum_3','MeanTemp_u20Sum_7','SsMean_1', 'SsMean_3', 'SsMean_7']\n",
    "\n",
    "SFE = main_with_spores[to_keep].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace crop nan's with 'unknown' category\n",
    "SFE['Crop'] = SFE['Crop'].fillna('unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinTemp_3       0\n",
       "MinTemp_1       0\n",
       "MeanTemp_u20    0\n",
       "MaxTemp_u20     0\n",
       "SsMean_7        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SFE.isnull().sum().sort_values().tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add date features\n",
    "SFE['Date'] = pd.to_datetime(SFE['Date'])\n",
    "SFE['month'] = pd.to_datetime(SFE['Date']).dt.month\n",
    "SFE['year'] = pd.to_datetime(SFE['Date']).dt.year\n",
    "SFE['YearWeek'] = pd.to_datetime(SFE['Date']).dt.isocalendar().week\n",
    "\n",
    "# create log of target (add 1 to get around 0 errors)\n",
    "SFE['logSsMean'] = np.log(SFE.SsMean + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "SFE.to_csv('SFE_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now create FE and JFE dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Provincial Weather DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "# wanted to keep leaf wetness and soil water content from field weather stations but too many missing values\n",
    "to_keep = ['FieldID','Date','JDay','SsMean','Crop',\n",
    " 'MinTemp_y','MaxTemp_y','MeanTemp_y','MeanRH_y','Precip','Wind2','Wind10','Wind10Dir','Frost','MeanDP_y',\n",
    " 'MinTemp_uDP','MinTemp_u20','MaxTemp_uDP','MaxTemp_u20','MeanTemp_uDP','MeanTemp_u20','MinTemp_1','MinTemp_3',\n",
    " 'MinTemp_7','MaxTemp_1','MaxTemp_3','MaxTemp_7','MeanTemp_1','MeanTemp_3','MeanTemp_7','MeanRH_1',\n",
    " 'MeanRH_3','MeanRH_7','Precip_1','Precip_3','Precip_7','Wind2_1','Wind2_3','Wind2_7','Wind10_1',\n",
    " 'Wind10_3','Wind10_7','Wind10Dir_1','Wind10Dir_3','Wind10Dir_7','Frost_1','Frost_3','Frost_7',\n",
    " 'MeanDP_1','MeanDP_3','MeanDP_7','PrecipSum_1','PrecipSum_3','PrecipSum_7','MinTemp_uDPSum_1','MinTemp_uDPSum_3',\n",
    " 'MinTemp_uDPSum_7','MaxTemp_uDPSum_1','MaxTemp_uDPSum_3','MaxTemp_uDPSum_7','MeanTemp_uDPSum_1','MeanTemp_uDPSum_3',\n",
    " 'MeanTemp_uDPSum_7','MinTemp_u20Sum_1','MinTemp_u20Sum_3','MinTemp_u20Sum_7','MaxTemp_u20Sum_1','MaxTemp_u20Sum_3','MaxTemp_u20Sum_7',\n",
    " 'MeanTemp_u20Sum_1','MeanTemp_u20Sum_3','MeanTemp_u20Sum_7']\n",
    "\n",
    "FE = main[to_keep].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace crop nan's with 'unknown'\n",
    "FE['Crop'] = FE['Crop'].fillna('unknown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FieldID             0\n",
       "Date                0\n",
       "PrecipSum_3         0\n",
       "PrecipSum_1         0\n",
       "MeanDP_7            0\n",
       "MeanDP_3            0\n",
       "MeanDP_1            0\n",
       "Frost_7             0\n",
       "Frost_3             0\n",
       "Frost_1             0\n",
       "Wind10Dir_7         0\n",
       "Wind10Dir_3         0\n",
       "Wind10Dir_1         0\n",
       "Wind10_7            0\n",
       "Wind10_3            0\n",
       "Wind10_1            0\n",
       "Wind2_7             0\n",
       "PrecipSum_7         0\n",
       "MinTemp_uDPSum_1    0\n",
       "MinTemp_uDPSum_3    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FE.isnull().sum().sort_values(ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add date features\n",
    "FE['Date'] = pd.to_datetime(FE['Date'])\n",
    "FE['month'] = pd.to_datetime(FE['Date']).dt.month\n",
    "FE['year'] = pd.to_datetime(FE['Date']).dt.year\n",
    "FE['YearWeek'] = pd.to_datetime(FE['Date']).dt.isocalendar().week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create log of target (add 1 to get around 0 errors)\n",
    "FE['logSsMean'] = np.log(FE.SsMean + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "FE.to_csv('FE_data2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lighthouse",
   "language": "python",
   "name": "lighthouse"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
